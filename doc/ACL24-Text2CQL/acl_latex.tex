% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% JackDann: Use Chinese Character support package
\usepackage{CJKutf8}

% JackDann: Use Algorithmic support package
\usepackage{algpseudocode}
\usepackage{algorithm}

\usepackage{fancyvrb}

% Remove the "review" option to generate the final version.
\usepackage{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

% Jack Dann
% To use Dui and Cuo, use this package XD
\usepackage{bbding}

% Jack Dann
% To use picture and tables use these package
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{caption}
\usepackage[bottom]{footmisc} % Handle footnotes at the bottom of the page
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{From Text to CQL: Bridging Natural Language and \\Corpus  Search Engine}

\author{Luming Lu$^{1*}$ \
  Jiyuan An$^{1*}$ \
  Yujie Wang$^{2}\thanks{\ \ Equal contribution.}$ \ \ 
  Liner yang$^{1}\thanks{\ \ Corresponding author.}$ \ \  
  Cunliang Kong$^{3}$ \ 
  \textbf{Zhenghao Liu}$^{4}$ \\
  \textbf{Shuo Wang}$^{3}$ \ 
  \textbf{Haozhe Lin}$^{3}$ \ 
  \textbf{Mingwei Fang}$^{1}$\ 
  \textbf{Yaping Huang}$^{2}$ \ 
  \textbf{Erhong Yang}$^{1}$ \\
  $^1$Beijing Language and Culture University, China \quad   
  $^2$Beijing Jiaotong University, China \\
  $^3$Tsinghua University, China \quad 
  $^4$Northeastern University, China \\
  }


\begin{document}
\begin{CJK*}{UTF8}{gbsn}
\maketitle
\begin{abstract}
Natural Language Processing (NLP) technologies have revolutionized the way we interact with information systems, with a significant focus on converting natural language queries into formal query languages such as SQL. However, less emphasis has been placed on the Corpus Query Language (CQL), a critical tool for linguistic research and detailed analysis within text corpora. The manual construction of CQL queries is a complex and time-intensive task that requires a great deal of expertise, which presents a notable challenge for both researchers and practitioners. This paper presents the first text-to-CQL task that aims to automate the translation of natural language into CQL. We present a comprehensive framework for this task, including a specifically curated large-scale dataset and methodologies leveraging large language models (LLMs) for effective text-to-CQL task. In addition, we established advanced evaluation metrics to assess the syntactic and semantic accuracy of the generated queries. We created innovative LLM-based conversion approaches and detailed experiments. The results demonstrate the efficacy of our methods and provide insights into the complexities of text-to-CQL task.
\end{abstract}





\section{Introduction}

Natural Language Processing~(NLP) technologies have significantly improved our interaction with information systems, enabling a more intuitive and effective interface to communicate with computers. Among these advances, the conversion of natural language queries into query languages, such as Structured Query Language~(SQL) for databases, has been a focal point of research. The exploration of linguistic corpora has benefitted significantly from advances in query languages, enabling researchers and practitioners to navigate and analyze text corpora efficiently. While several query languages, such as SQL for databases and various Domain-Specific Languages (DSLs) for other applications, have seen extensive study and application, the focus on Corpus Query Language (CQL) has been relatively less pronounced. Existing research has extensively explored Text-to-SQL\citep{zhong2017seq2sql, liu2022tapex, yu-etal-2018-spider, li2023resdsql, gao2023texttosql, pourreza2023dinsql, dong2023c3, li2023can} and Text-to-DSL\citep{wang2023grammar, staniek2023texttooverpassql} tasks, demonstrating the feasibility and efficiency of translating natural language instructions into formal query statements to interact with databases and information systems.

\begin{figure}[t]
    \centering
    \includegraphics[width=3in]{CQL.drawio.pdf}
    \caption{Example of text-to-CQL . Given any input natural language query description, the model is expected to convert it into the corresponding Corpus Query Language (CQL) and the generated CQL should be able to be accurately executed by the Corpus Engine. The CQL uses symbols (in green) with a small number of CQL uses symbols (green) and a small number of keywords (purple) to construct queries, and allows to specify names (blue) for tokens to constrain relationships between tokens. The corpus search engine will return a query execution result.}
    \label{fig:cql}
\end{figure}

% \footnotetext{Execution Results are the results returned by Blacklab when executing the CQL on the En-Wiki corpus.}

CQL is vital for linguistic research as it offers a nuanced approach to querying annotated text corpora, allowing for sophisticated searches based on linguistic features. This capability is crucial for conducting detailed linguistic analysis and supports a wide range of research activities in NLP and computational linguistics. However, crafting CQL queries manually is a time-consuming and error-prone process that requires a high level of expertise in both the query language and the specific annotations of the corpus being queried.

This work introduces the task of text-to-CQL. This task aims to bridge the gap between natural language descriptions and their corresponding CQL representations, facilitating more accessible and efficient interactions with linguistic corpora with rich linguistic annotations. However, unlike its counterparts in text-to-CQL task, the text-to-CQL task faces unique challenges, including a scarcity of dedicated training data and the intricate syntax and semantics of CQL, which are not easily handled by even the most advanced generative models, such as GPT-4, without specialized training and adaptation.
Models need not only to understand the semantics of natural language descriptions, but also to navigate the complexities of linguistic annotations and the specific query constructs of CQL.

This work aims to address these challenges by proposing a comprehensive framework for the text-to-CQL task. We introduce a novel dataset specifically curated for this task, along with methodologies and evaluation metrics tailored to the unique requirements of CQL query generation. Our contributions include the creation of a large-scale dataset that encompasses a wide range of linguistic phenomena and query types, the development of models that adapt large language models and introduce new LLM-based approaches for text-to-CQL task, and the establishment of evaluation metrics that go beyond traditional measures to assess the syntactic validity and semantic correctness of the generated queries.

In summary, our key contributions are as follows:
\begin{itemize}
    \item A large-scale, diverse dataset for text-to-CQL task, providing a benchmark for model evaluation.
    \item A series of LLM-based text-to-CQL methodologies, including both prompt engineering and fine-tuning pretrained language models.
    \item New evaluation metrics designed to accurately reflect the complexities of the text-to-CQL task, focusing on syntactic validity and semantic correctness.
    \item Comprehensive experiments and analysis that highlight the effectiveness of our proposed methods and offer insights into the challenges of text-to-CQL conversion.
\end{itemize}

We will release all our code and datasets for research purposes on Github.


\section{Background}


 Corpus Query Language (CQL) is a query language specifically used to query a corpus with the linguistic features required by users. The CQL utilized in this article is mainly related to the BlackLab~\citep{blacklab}\footnote{https://github.com/INL/BlackLab} corpus Query Language\footnote{ The existing systems for Corpus Query Languages are offshoots of the Corpus Query Language Processor (CQP)~\citep{hardie2012cqpweb} query language, which is a suite of languages designed for the retrieval of lexical information. 
}. 
 
 Numerous corpus search tools endorse and employ CQL, including well-known platforms such as CQPweb\footnote{https://cwb.sourceforge.io/cqpweb.php}, Sketch Engine\footnote{https://www.sketchengine.eu/}, and BlackLab, among others. Some examples of CQL are shown in Table \ref{tab:CQLexample}.

\begin{table*}
    \centering
    \begin{tabular}{p{5em}p{15em}p{15em}} 
    \hline
    \textbf{Type} & \textbf{CQL} &\textbf{NL} \\
    \hline
    Simple & \verb|[lemma="teapot"]| &Find the lemma teapot.\\
    Within & \verb|[pos="N.*"]| \verb|within| \verb|[pos="VB.*"]| \verb|[]|\verb|{0,5} |\verb|[pos="VB.*"]| & Searches for nouns that appear between two verbs to be, the verbs are at a distance of max. 5 tokens from each other.\\
    Condition & \verb|1:[] 2:[] :: 1.pos = 2.pos| & Find any two tokens whose tag is the same.\\
    \hline
    \end{tabular}
    \caption{Example of the Corpus Query Language. The above examples and explanations are all from the Sketch Engine documentation.}
    \label{tab:CQLexample}
\end{table*}

% \footnotetext{https://www.sketchengine.eu/documentation/corpus-querying/}
 

\subsection{CQL Statementes}
\citet{pourreza2023dinsql} categorized SQL into simple, complex, and nesting classes, delineating distinctions in sentence structure complexity within the Text-to-SQL task. Analogously, CQL can be classified into three categories based on keywords, as distinct keywords induce alterations in the CQL structure.

\textbf{Simpe Query.} In CQL, users possess the ability to formulate queries that target the desired corpus by using sequential associations among the tokens. For example, to retrieve instances of research categorized as nouns within the corpus, a user can use the following CQL:
\verb|[word='research' & pos='NN']|

In the above example, we employed a corpus aligned with the Penn Treebank (PTB) \citep{ptb} part-of-speech system. The model's capacity to accurately associate the lexical properties of natural language representations with the appropriate lexical labels represents a potential challenge.

\textbf{Within Query.} As shown in Table \ref{tab:CQLexample}, the "within" syntax serves to partition a CQL statement into two sub-queries, restricting the target retrieved in the initial portion to the scope delineated in the subsequent segment. Typically, "within" is accompanied by a subquery with a larger maximum target length or certain XML structure. 

\textbf{Condition Query.} A condition statement is employed to compare the tokens with each other and to impart additional options to individual tokens. All attributes of a token are eligible for comparison within a condition.



% JackDann@2024-1-16
% Section Dataset
\section{Dataset Construction }

Given a parallel dataset of natural language descriptions and CQL queries $D = \left\{(X_i, Y_i)\right\}_{i=1}^N$, where $X_i = \left\{w_1, w_2, \ldots, w_{n_1}\right\}$ is a query described in natural language and $Y_i = \left\{t_1, t_2, \ldots, t_{n_2}\right\}$ is the CQL corresponding to the NL. $n_1$ represents the length of the natural language description while $n_2$ represents the length of the CQL query. The goal of the text-to-CQL task is to train a model that converts a natural language query description into a query language. This also means that the model needs to have the ability to extract key information from a natural language description and combine it into CQL with the correct syntax. For this purpose, constructing a dataset from natural language to CQL is necessary.

\subsection{Corpus Collection}
We employed two distinct corpora for our study, one in Chinese and the other in English. Both corpora were annotated using Stanford Corenlp.

\textbf{TCFL Textbook.} We collected the main teaching materials to teach Chinese as a foreign language on the market and constructed the TCFL Textbook.


\begin{table}[h!]
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Dataset} & \textbf{Sentences} & \textbf{Tokens}\\
\midrule
TCFL Textbook & 578.4 k & 7.7 M  \\
EnWiki & 138.6 M & 3.1 B  \\
\bottomrule
\end{tabular}
\caption{The token and sentiment size of the corpus we used.}
\label{tab:corpusScale}
\end{table}



\textbf{EnWiki.} We use the EnWiki \citep{enwiki}\footnote{ https://dumps.wikimedia.org/} corpus and clean and extract the text in it using wiki-extractor\footnote{https://github.com/attardi/wikiextractor}\citep{Wikiextractor2015}.  In Table \ref{tab:corpusScale}, We show the size of the cleaned dataset.

Our text-to-CQL dataset is divided into two parts: Chinese NL-CQL pairs based on the TCFL textbooks and English NL-CQL pairs based on Wiki.
\subsection{CQL Generation Strategies}

Certain conventional dataset construction methods, such as the data mining techniques employed in WikiTable\citep{wikitable}, are precluded due to the limited availability of pertinent information on the Internet. Consequently, we develop a novel data collection approach grounded in Chinese collocation extraction.

\subsubsection{Collocation Extraction}

Our approach to data augmentation is based on Chinese Collocation Extraction\citep{hucollocation}. An example of Chinese collocation extraction is shown in table \ref{tab:collexample}. The collocation set extraction methodology takes advantage of both surface and dependency relation knowledge, along with statistical methods. Furthermore, we incorporate the enhanced Chinese dependency\citep{yujingsi} to improve the efficacy of collocation extraction. Specifically, our process involves initially employing Stanford CoreNLP\citep{corenlp} for the dependency analysis of sentences within the corpus. Subsequently, enhanced dependencies are introduced, and collocations are extracted from the entire corpus. Finally, a random selection is made from the extracted collocations and applied in classified CQL templates.



\subsubsection{CQL Template}
As depicted in Table \ref{tab:CQLexample}, the CQL queries can be categorized into three distinct types: simple, within, and condition. In alignment with these three types of CQL, we established distinct templates that are tailored for each type.

\textbf{Simple Statements. }
Given that the collocations extracted from the corpus consist of word combinations that exceed two words, our post-extraction procedure involves traversing the word sequence.
Initially, we randomly assign a set of conditions for each token. For any token (such as the noun 'book'), we randomly convert it into the following forms:

1. Simple word query (W).

2. Simple part-of-speech query (P).

3. Query its word and POS at the same time. The logical relationship between the two conditions is randomly chosen from \emph{AND} (WAP) or \emph{OR} (WOP).

4. Query two words at the same time and there is an OR relationship between them (WW). In this case, we also randomly restrict its part of speech with an AND relation (WWP). Another candidate word is selected based on the synonyms specified in the synonym forest.
\begin{table}
    \centering
    \begin{tabular}{p{5em}p{12em}} 
    \hline
    \textbf{Query Type} & \textbf{CQL} \\
    \hline
    W& \verb|[word='book']| \\
    P& \verb|[pos='NN']|\\
    WOP& \verb^[word='book'|pos='NN']^\\
    WAP& \verb^[word='book'&pos='NN']^\\
    WW& \verb^[word='book'|word=^ \verb^'notebook']^\\
    WWP& \verb^[(word='book'|word=^ \verb^'notebook')&pos='NN']^\\

    \hline
    \end{tabular}
    \caption{An example of a CQL Token Queries transformed from Token extracted from a corpus, containing 6 random transformations: simple Word Query (W), Simple Pos Query (P), Word and Pos Query (WAP), Word or Pos Query (WOP), Word or Word query (WW) and Words with Pos Query (WWP)}
    \label{tab:simpleTokenExample}
\end{table}

Examples are shown in Table \ref{tab:simpleTokenExample}. After converting the word in each collocation to CQL, we randomly add empty tokens to it as shown in Algorithm \ref{alg:cap}, where the $mutate$ function refers to the process of converting the collocation word to a CQL token as described in this section, and the $insert\_null\_token$ method randomly adds an unrestricted token to the end of a CQL and assigns it a random number of repetitions or quantifiers using regular expressions.



\begin{algorithm}
\caption{Generation of simple CQL}\label{alg:cap}
\begin{algorithmic}
\State \textbf{Input} $Collocation = \left\{ w_1, w_2, \ldots, w_n \right\}$
\State \textbf{Output} CQL
% \Ensure $y = x^n$
% \State $y \gets 1$
% \State $X \gets x$
% \State $N \gets n$
\State CQL$\gets None $ 
\While{$i \neq 0$}
\If{$freq(w_i) \leq 5 $}
    \State $Abandon()$
\Else
    \State CQL.append(Mutate($w_i$))
    \If{$w_{i+1} = "X"$}
        \State CQL.insert\_null\_token()
        \State $i \gets i + 1$    
    \Else
        \If{$random\_number < 0.5$} 
            \State CQL.insert\_null\_token()
        \EndIf
    \EndIf
    \State $i \gets i + 1$
\EndIf
\EndWhile
\end{algorithmic}
\end{algorithm}

\textbf{Within Statements. }
Two potential subqueries are permissible following the keyword \verb|within|:
 1) \textbf{Simple CQL Subquery.} This causes the corpus searching engine to search for CQL before the \verb|within| keyword within the specified subquery scope. 
 2) \textbf{Structure.} One may utilize XML Structure to confine the query scope to the specified XML domain, with strict prohibition on extending beyond the boundaries defined by the tags. 

For both cases, we randomly apply one of them. On the one hand, two CQLs are generated through collocation analysis, in which the maximum token length they can reference is examined. Then the shorter one is placed preceding the \verb|within| keyword. On the other hand, we randomly specify a certain level of XML format and place it after the \verb|within|. 

We also generate muiti "within" statements. However, nesting multiple levels of queries may lack meaningful interpretation and pose challenges in natural language description. Furthermore, XML structures are commonly segmented at the sentence level and beyond, rendering queries across XML structures practically insignificant. Consequently, we restrict the generation of nested queries to those comprising two "within" keywords, with the XML structure query positioned at the end of the query (representing the highest-priority decision). An example of our generated \emph{within} CQL is shown in Table \ref{tab:within_example}




\textbf{Condition Statements. }
we extract the analytical outcomes of all sentences within the corpus. From these results, we identify token pairs within sentences where parts of speech or words share equality. Subsequently, sentences containing such token pairs are randomly selected, and CQL with condition syntax is generated based on these equivalence relationships. Given that the collocation-based method is no longer applicable to CQL with equivalence relationships, our consideration is limited to scenarios that involve the embedding of CQL within the XML structure in condition statements. An instance of our generated condition CQL is shown in Table~\ref{tab:condition_example}.


\subsection{Annotation}

We create the text-to-CQL dataset \textbf{TCQL} with manual annotation. To ensure clarity and precision in natural language descriptions, we implemented a training and selection process for our annotators. Of the initial pool of 14 recruited annotators, we assessed their abilities and ultimately retained eight annotators for subsequent annotation tasks. These annotators have undergraduate degrees and are familiar with both computer science and linguistics.

We perform 4 rounds of labeling for each dataset. First, for the CQLs that have been generated, we perform the initial labeling using the OpenAI GPT-4 API~\citep{openai2023gpt4}. We ask GPT-4 to generate the demand text based on a given CQL and prompt it with the necessary information. Then, we ask the annotator to perform 2 rounds of re-labeling to revise the errors in the results of the initial annotation. Finally, two reviewers who are well-versed in CQL syntax are responsible for reviewing the annotation results again. The size of the labeled data set is shown in Table \ref{tab:combined_statistics}.

\begin{table}[h!]
\centering
\begin{tabular}{lccc}
\toprule
 & \textbf{Train} & \textbf{Dev} & \textbf{Test} \\
\midrule
\textbf{Simple} & 5,631 & 805 & 1,609 \\
\textbf{Within} & 2,332 & 334 & 667 \\
\textbf{Condition} & 1,399 & 199 & 401 \\
\textbf{All} & 9,362 & 1,328 & 2,677 \\
\bottomrule
\end{tabular}
\caption{Combined Classification Statistics for TCQL datasets.}
\label{tab:combined_statistics}
\end{table}

\section{Methodology}
In the construction of the text-to-CQL dataset, we implemented five distinct methodologies, encompassing approaches based on the In Context Learning (ICL) method and approaches using pretraining or fine-tuning pretrained language models.

\subsection{In-Context Learning (ICL) Methods}

We investigate three classifications of Large Language Model (LLM) prompt methods to assess the efficacy of LLMs in text-to-CQL tasks.

\textbf{Documentation Prompt (DP).} Furnish the LLM with a CQL tutorial created by human experts, derived from tutorials accessible in Sketch Engine\citep{sketchengine1,sketchengine2} and Blacklab\citep{blacklab} Documentation. Within the tutorial, we elucidate the syntax of CQL using natural language and furnish illustrative instances, sourced from the tutorial documentation.

\textbf{Few-shot ICL.} We adhere to the methodology outlined by \citet{sun2023battle} and three sets of experiments with different numbers of examples were set up. In each group of examples, we set an example for each of the three types of CQL. 1-Shot Learning (1SL) and 3-Shot Learning (SL) mean that we embed one or three groups of examples in the prompt. Prompt details can be found in the Appendix \ref{appendix:prompts}.

\subsection{Fine-tuning PLM Methods. }  

Models equipped with an encoder-decoder architecture are aptly suited the text-to-CQL task. This category encompasses several models, including BERT\citep{devlin2018bert}， T5\citep{t5}, BART\citep{bart}, and  GPT\citep{radford2019gpt}, among others. this study prioritizes BART due to its integrated encoder-decoder architecture. Furthermore, BART's foundation on a denoising autoencoder pre-training paradigm potentially enhances its proficiency in natural language comprehension and structured query generation, as evidenced by preliminary experimental findings.

For the generation of CQL queries from Chinese texts, this research employed the BART-Chinese model \citep{shao2021bartlargechinese}. We leverage the most expansive `Large' size model available. respectively. Two distinct methodologies were applied for the fine-tuning of the pre-trained language model: Prefix-tuning and Full Model Fine-tuning. The findings indicate that, within the context of the Chinese text-to-CQL task, prefix-tuning yielded superior results. Conversely, for the English text-to-CQL task, full model fine-tuning demonstrated enhanced performance. Appendix \ref{appendix:more} gives more results of PLM performance on this task


\subsection{Metrics}

In this section, we draw upon prior research in the domain of Text-to-SQL, as well as relevant Text-to-Code evaluation metrics, to introduce the four evaluation metrics employed in our study.

\subsubsection{Exact Match (EM)}

Exact Match (EM) is used to evaluate whether the generated SQL query matches exactly the human-annotated standard query. Specifically, the EM metric measures whether the generated SQL query agrees with the reference query without any differences. 
However, execution accuracy may create false positives for CQL queries that are semantically identical but have different forms\citep{yu2018spider, deng-etal-2022-recent}.  

\subsubsection{Valid Accuracy (VA)}

We introduce the Valid Accuracy (VA) metric, which is designed to assess the syntactic correctness of the generated code concerning the CQL grammar. The VA metric provides insights into the model's ability to generate syntactically sound code structures.

\subsubsection{Execution Accuracy (EX)}
Execution Accuracy (EX) metrics are used to evaluate how well the generated CQL query executes on the corpus Engine. It determines whether the generated SQL query executes correctly and returns the desired result \footnote{In our experimental setup, we employ BlackLab as the execution engine for CQL and ascertain the congruence of the corpus retrieval results.}.

\subsubsection{CQLBLEU}

Inspired by~\citet{ren2020codebleu}, we propose new CQLBLEU metrics. This metric is used to assess the similarity between the CQL generated by the model and the reference CQL. Specifically, CQLBLEU is a combination of BLEU\citep{papineni2002bleu} and semantic similarity metrics. Given an candidate CQL $Q_{c}$ and a reference CQL $Q_{r}$, CQLBLEU is defined as:
\begin{equation}
\begin{aligned}
\text{CQLBLEU}(Q_{c},Q_{r}) =& \alpha\cdot \text{BLEU}(Q_{c},Q_{r}) \\ &+ \beta\cdot \text{TS}(Q_{c},Q_{r})
\end{aligned}
\end{equation}
where BLEU stands for BLEU metrics and TS stands for the AST tree similarity which can be a semantic similarity metric. The metric is computed based on the AST generated after syntactic parsing:
\begin{align}
    &T_c = \text{Parse}(Q_{c}) \\
    &T_r = \text{Parse}(Q_r) \\
    &\text{TS}(Q_{c},Q_{r}) = \text{Sim}(T_c,T_r) 
\end{align}
where $T_c$ and $T_r$ are the CQL AST of $Q_{c}$ and $Q_r$ parsed by Blacklab. The Sim function compares each node in the AST of $Q_{c}$ by itself and its direct children for the presence of $Q_r$:
\begin{align}
    \text{Sim}(T_c, T_r)=     \frac{\sum_{n_c \in \text{N}(T_c)} \text{Match}(n_c, T_r)}{|\text{N}(T_c)|}
\end{align}
where $\text{N}(T)$ represents the set of non-leaf nodes in tree $T$, and $\text{Match}(n_c, T_r)$ is a function that returns 1 if a node $n_c$ from $T_c$ and its direct children have a matching signature in $T_r$, and 0 otherwise. The matching criterion for a node $n_c$ with signature $s(n_i) = (\text{N}(n_c), \text{C}(n_c), \text{K}(n_c))$ against $T_r$ is defined as follows:
\begin{equation}
\begin{aligned}
    & \text{Match}(n_i, T_r) = \\ &\begin{cases}
    1, & \text{if } \exists n_r \in \text{N}(T_r) : s(n_i) = s(n_r) \\
    0, & \text{otherwise}
    \end{cases}
\end{aligned}
\end{equation}

The coefficients $\alpha$ and $\beta$ in the definition of CQLBLEU allow for balancing the contribution of syntactic similarity, as measured by BLEU, and semantic similarity, as measured by $TS$, to the overall metric. In our work, we choose $\alpha = 0.5$ and $\beta = 0.5$. 

\section{Analysis}

\begin{table*}[ht]
\centering
\begin{tabular}{lp{2em}cccccccc}
\toprule
\multirow{2}{*}{Model} & \multirow{2}{*}{Settings} & \multicolumn{4}{c}{TCFL Textbook} & \multicolumn{4}{c}{EnWiki} \\
\cmidrule(lr){3-6} \cmidrule(lr){7-10}
      &          & EM & VA & EX & CQLBLEU & EM & VA & EX & CQLBLEU \\
\midrule
BART-Chinese & - & 46.52 & 80.46& 50.95 & 72.95 & - & -& -& - \\
BART-English         & - & - & - & - & - & 37.58 & 81.74 & 44.30 & 82.13 \\
GPT-4 & DP & 35.17 & 77.52 & 51.79 & 74.95 & 14.93 & 75.37 & 24.49 & 67.63 \\
GPT-4 & 1SL & 47.81 & 81.84 & 62.71 & 82.22 & 43.31 & 82.24 & 51.87 & 82.93 \\
GPT-4 & 3SL & 67.49 & 90.28 & 77.85 & 91.83 & 58.24 & 89.74 & 65.53 & 89.93\\
\bottomrule
\end{tabular}
\caption{Experiment results. The table contains the results of four evaluation metrics: Exact Match (\textbf{EM}), Valid Accuracy (\textbf{VA}), Execution Accuracy(\textbf{EX}), and \textbf{CQLBLEU}. We choose the BART-Large model and use its Chinese branch to fit our Chinese dataset.}
\label{tab:model_performance}
\end{table*}

% \begin{table}[h!]
% \centering
% \begin{tabular}{lp{5em}p{5em}}
% \toprule
% \textbf{Dataset} & \textbf{Query Language} & \textbf{Number of Instances} \\
% \midrule
% GeoQuery & SQL & 880 \\
% NLmaps & MRL & 2,380 \\
% NLmaps v2 & MRL & 28,609 \\
% WikiSQL & SQL & 81,654 \\
% Spider & SQL & 10,181 \\
% OverpassNL & OverpassQL & 8,352 \\
% \hline
% TCQL & CorpusQL & 13,367
% \\
% \hline
% \end{tabular}
% \caption{Comparison of the size of the TCQL dataset with datasets used in other natural languages to query language tasks. The TCQL dataset is based on manual annotation, contains both Chinese and English languages, and has a large size.}
% \label{tab:datasetCompare}
% \end{table}TABLE 9

\subsection{LLM capability assessment}
In our LLM-based ICL experiments, we found three significant features of LLM for this task:

\textbf{LLM by itself is almost incapable of writing CQL correctly}. In our early test, LLM shows low performance of several methods for each of the three CQL classifications: simple, within, and conditional. LLM did not perform better than PLM even when Documentation Prompt (DP) was provided. This may be due to the fact that the training data that LLM was exposed to may have contained fewer CQL examples, and these examples were mostly focused on simple classification, and not much on the other two classifications, which are more flexible and broader in application scenarios.

\textbf{LLM is much better at learning from examples.} We experimented with having LLM learn CQL knowledge from documents written by human experts (DP) and having LLM learn from examples given CQL-NL pairs (1SL and 3SL). In the DP approach, there is still a large gap between LLM and finetuned PLM, which may mean that LLM is not as efficient at reading documents that are more easily understood by humans. The results show that CQL can effectively understand the syntax of the query language in fewer samples. This is consistent with \citet{staniek2023texttooverpassql}'s conclusion.

\textbf{LLM understands the semantics expressed in human language}. In most cases, LLMs achieve high CQLBLEU scores even if they are not given detailed hints about the CQL syntax or if their execution results do not meet expectations. This means that LLM writes answers that are closer to human answers in terms of semantic similarity and text. This ability of LLM can continue to be enhanced with more hints or examples. This also confirms that LLM learns not only formal knowledge from examples but also semantic information.

\subsection{PLM Performance Analysis}

\subsubsection{Performance of PLM on different languages}
Based on the experimental results described in the previous section, the performance of the same large-sized BART model shows differences in the text-to-CQL tasks for both English and Chinese languages. Beyond the differences in model performance due to the language used for fine-tuning, we believe a more significant reason is the addition of the "lemma" attribute in English CQL compared to Chinese. In addition to "word" and "pos" in Chinese queries, English queries also include "lemma," requiring the model to learn an additional attribute name. Furthermore, the forms of words and their lemmas are quite similar in natural language expression and are often mixed in actual human queries. The model exhibits similar behavior, where the predicted CQL queries differ from the gold standard only in the attribute names "word" and "lemma," which is a very common type of error occurrence.

\subsubsection{Performance of PLM on different query difficulties}

To better assess the performance of our proposed model, we categorized CQL queries into three levels of difficulty based on human habits in writing CQL queries. According to our intuition, the difficulty of generating CQL queries from text for the model should follow the order: Simple < Within < Condition. However, the model's performance in some cases deviated from our expectations, showing a significantly better performance on \emph{condition} type than on \emph{within} type (for example, when using the DP method). To elucidate the reasons behind this phenomenon, we provide detailed statistical data from the dataset, as shown in Table \ref{tab:enstat} and Table \ref{tab:zhstat} in Appendix \ref{appdix:dataset}, We found that in terms of the character length of CQL queries and the number of constraint conditions, \emph{Within} type far exceeds Condition type, implying that natural language inputs of the Within type lead to the generation of the longest CQL queries with the most constraint conditions, which typically signifies a higher probability of errors. Conversely, \emph{condition} type demonstrated more complex query logic, but since it involves more non-constraint word queries, the primary challenge it poses to the model is the understanding of the logic in the natural language input rather than longer CQL queries and more constraint conditions.





\section{Related Work}
\label{sec:related_work}

\subsection{Text-to-SQL}
\label{sec:text2sql}

Text-to-SQL task, a key research area, involves translating natural language questions into SQL queries. 
Seq2SQL~\citep{zhong2017seq2sql} is a notable model in this field, utilizing policy-based reinforcement learning to accurately generate SQL queries, particularly focusing on the unordered nature of query conditions. 
It excelled in both execution and logical form accuracy on the WikiSQL dataset.
In the same data set, TAPEX~\citep{liu2022tapex}, an execution-centric table pretraining approach that learns a neural SQL executor over a synthetic corpus, achieved the state-of-the-art results.

The Spider~\citep{yu-etal-2018-spider} dataset furthered Text-to-SQL research by presenting a complex, cross-domain semantic parsing challenge. 
It features varied SQL queries and databases, pushing models to adapt to new structures and databases. 
RESDSQL~\citep{li2023resdsql} introduced a ranking-enhanced encoding and skeleton-aware decoding framework that effectively decouples schema linking and skeleton parsing, demonstrating improved parsing performance and robustness on the Spider dataset and its variants.

Recent advances in large language models (LLMs), such as GPT-4~\citep{openai2023gpt4} and Claude-2, have also shown impressive results in this domain~\citep{gao2023texttosql, pourreza2023dinsql, dong2023c3}. 
To our knowledge, most previous benchmarks, including Spider and WikiSQL, focused on database schemas with limited rows, creating a gap between academic studies and real-world applications. 
To bridge this gap, the BIRD~\citep{li2023can} benchmark was introduced, providing a comprehensive text-to-SQL dataset that emphasizes the challenges of dealing with dirty and noisy database values, grounding external knowledge, and ensuring SQL efficiency in massive databases.

However, adapting these methods from Text-to-SQL to text-to-CQL isn't straightforward, primarily because of the scarcity of training data for text-to-CQL. 
This challenge motivated the proposal of this paper.

\subsection{Text-to-DSL}
\label{sec:text2dsl}

The field of generating Domain-Specific Languages (DSLs) from natural language, known as Text-to-DSL, has seen a surge in interest, primarily due to the emergence of LLMs capable of understanding and generating structured languages. 
A notable approach in this area is Grammar Prompting~\citep{wang2023grammar}, which leverages Backus–Naur Form (BNF) grammars to provide LLMs with domain-specific constraints and external knowledge. 
This method has shown promise across various DSL generation tasks, including semantic parsing and molecule generation.

Text-to-OverpassQL~\citep{staniek2023texttooverpassql} focused on generating Overpass queries from natural language. 
This task is particularly challenging due to the complex and open-vocabulary nature of the Overpass Query Language (OverpassQL). 
\citet{staniek2023texttooverpassql} proposed the OverpassNL dataset and established task-specific evaluation metrics.

\section{Conclusion}

In this paper, we introduce a novel task, text-to-CQL, aimed at converting natural language input into Corpus Query Language (CQL). This task holds significant relevance for corpus development and research, sharing certain similarities with existing text-to-query language tasks. The text-to-CQL task, however, presents distinctive challenges owing to its unique syntax and limited availability of publicly accessible resources. To support research in this domain, we propose TCQL—a template-based generation approach for creating text-to-CQL datasets. To ensure the authenticity of the dataset, we build the data based on NLP tasks such as collocation extraction and lexical labeling. We use this dataset to test the performance of several state-of-the-art models, propose a new evaluation metric, CQLBLEU, based on N-gram similarity and AST similarity, and build a baseline for the tasks with reference to several commonly used metrics in Text-to-SQL.We evaluate the results in detail, revealing the strengths and weaknesses of the considered learning strategies. We hope that this contribution will positively impact corpus development and applications, advancing technology in both the realms of NLP and linguistics.



\label{sec:bibtex}

\section*{Limitations}
This work introduces a novel approach to converting natural language queries into Corpus Query Language (CQL) expressions. Despite its potential to significantly advance research in corpus linguistics and natural language processing, several limitations must be acknowledged:

\begin{itemize}
    \item Currently, the construction of the TCQL dataset used in this paper is based on automatically generated and manually labeled due to the lack of a large amount of raw CQL data generated from real human queries. Despite the fact that we have used a variety of methods to enhance its authenticity, it is still possible to generate queries that are not meaningful enough.
    \item The scalability of the proposed solution to longer text queries and its dependency on computational resources are concerns that may limit its applicability in resource-constrained settings.
\end{itemize}

Future research is encouraged to address these limitations, exploring the method's applicability to a wider range of languages, enhancing its scalability, and reducing its computational requirements.


% \section*{Ethics Statement}
% Scientific work published at ACL 2023 must comply with the ACL Ethics Policy.\footnote{\url{https://www.aclweb.org/portal/content/acl-code-ethics}} We encourage all authors to include an explicit ethics statement on the broader impact of the work, or other ethical considerations after the conclusion but before the references. The ethics statement will not count toward the page limit (8 pages for long, 4 pages for short papers).

% \section*{Acknowledgements}
% This document has been adapted by Jordan Boyd-Graber, Naoaki Okazaki, Anna Rogers from the style files used for earlier ACL, EMNLP and NAACL proceedings, including those for
% EACL 2023 by Isabelle Augenstein and Andreas Vlachos,
% EMNLP 2022 by Yue Zhang, Ryan Cotterell and Lea Frermann,
% ACL 2020 by Steven Bethard, Ryan Cotterell and Rui Yan,
% ACL 2019 by Douwe Kiela and Ivan Vuli\'{c},
% NAACL 2019 by Stephanie Lukin and Alla Roskovskaya, 
% ACL 2018 by Shay Cohen, Kevin Gimpel, and Wei Lu, 
% NAACL 2018 by Margaret Mitchell and Stephanie Lukin,
% Bib\TeX{} suggestions for (NA)ACL 2017/2018 from Jason Eisner,
% ACL 2017 by Dan Gildea and Min-Yen Kan, NAACL 2017 by Margaret Mitchell, 
% ACL 2012 by Maggie Li and Michael White, 
% ACL 2010 by Jing-Shin Chang and Philipp Koehn, 
% ACL 2008 by Johanna D. Moore, Simone Teufel, James Allan, and Sadaoki Furui, 
% ACL 2005 by Hwee Tou Ng and Kemal Oflazer, 
% ACL 2002 by Eugene Charniak and Dekang Lin, 
% and earlier ACL and EACL formats written by several people, including
% John Chen, Henry S. Thompson and Donald Walker.
% Additional elements were taken from the formatting instructions of the \emph{International Joint Conference on Artificial Intelligence} and the \emph{Conference on Computer Vision and Pattern Recognition}.

% Entries for the entire Anthology, followed by custom entries
\bibliography{custom}
% \bibliographystyle{acl_natbib}
% JackDann: To solve long text problem

% \appendix
% \label{sec:appendix}

% \section{Prompts of ICL Methods}
% \label{appendix:prompts}

% \subsection{Documentation Prompt (DP) Prompts}

% You are a CQL (Corpus Query Language) expert. Now there is a corpus query requirement. Please express it as the corresponding CQL: [查找紧接在"太"这个词之后的动词。] 
% Tip: In the corpus, the word attribute is word, and the part of speech attribute is pos. The part of speech tag system is CTB system. Each word is represented as \"[CONSTRAINTS]\", \"[]\" represents any word that is not specified.The format of the CONSTRAINTS is:
%  ```
%  key = 'value'
%  ```
%  or:
%  ```
%  key != 'value'
%  ```
%  Key could be `word` or `pos`.Positive closure or star closure can be performed on a single token, represented by `+` or `*`. like:
%  ```
%  []*
%  ```
%  You can also use `\{lower\_bound, upper\_bound\}` to indicate the range of repetition times, such as:
%  ```
%  [word='is']\{1,3\}
%  ```
%  It means "is" appears 1 to 3 times. If only `\{number of times\}` is used, it represents the exact number of repetitions.
%  Multiple sets of constraints in a token can be connected using logical operators `&` or `|`, and parentheses can also be used to specify the priority. value can be a regular expression or a specific string.
% The CTB part-of-speech system is as follows:
% AD: adverb
% AS: body marker
% BA: "把", "将", etc. in the sentence with "把"
% CC: coordinating conjunction
% CD: cardinal word
% CS: subordinating conjunction
% DEC: The punctuation word "的"
% DEG: possessive "的"
% DER: structural particle "得"
% DEV: structural particle "地"
% DT: qualifier
% ETC: Particle "等", "等等"
% FW: loanword
% IJ: interjection
% JJ: non-predicate adjective
% LB: "被" and "给" in long sentences with "被"
% LC: locative word
% M: quantifier
% MSP: Small particles, such as "所", "以", etc.
% NN: common noun
% NR: proper noun
% NT: time word
% OD: ordinal number
% ON: onomatopoeia
% P: preposition
% PN: pronoun
% PU: punctuation
% SB: "被" and "给" in short "被" sentences
% SP: Sentence ending essay
% VA: predicative adjective
% VC: Coupling verb
% VE: A verb expressing the concept of "to have" or "to have nothing"
% VV: common verb

% Tip: The within keyword in CQL can also mean "find the former within the latter range". like
% ```
% [word='is'] within [word='only'] []* [word='also']\n
% ```
% It can be understood as: a sentence containing "is" between "only" and "also".

% Tip: The corpus in CQL is stored in XML structure. You can use within <tag /> to specify that the query results are limited to the range of a certain tag. For example, the <s> tag represents a sentence, and the <p> tag represents a paragraph.

% Tip: You can use CONDITION syntax to specify conditional relationships between different tokens. Usage: Add `NAME:` before the required token, then use `::` at the end of the statement, and add the condition after that. For example, to find two consecutive words with the same part of speech:
% ```
% A:[] B:[] :: A.pos=B.pos
% ```
% Tip: The double colon and qualification of CONDITION syntax must be at the end of the CQL statement. If you want to use within and CONDITION at the same time, make sure within precedes the double colon.
% For example, find two words with the same part of speech in a sentence:
% ```
% A:[] []* B:[] within <s/> :: A.pos=B.pos
% ```
% Give the CQL directly, do not attach other information.

% \subsection{1-Shot ICL (1SL) Prompts}

% You are a CQL (Corpus Query Language) expert. Now there is a corpus query requirement. Please express it as the corresponding CQL.                                                                                                                                                                                                                  Here are some examples to help you understand the requirements and CQL grammar:                                                                                                    Example 1:                                                                                                                                                                         Requirement: Search for instances where the word 'attentive' is followed by any form of the word 'staff'.                                                                          CQL: [word = 'attentive'] [lemma = 'staff']                                                                                                                                                                                                                                                                                                                           Example 2:                                                                                                                                                                         Requirement: Find sentences where two consecutive adverbs that are not lemma "double" are followed by any word, and then the word "room".                                          CQL: [lemma != 'double' & pos = 'RB'] [lemma != 'double' & pos = 'RB'] [] [word = 'room']  within <s/>                                                                                                                                                                                                                                                                Example 3:                                                                                                                                                                         Requirement: The researcher is looking for instances where the lemma, or base form, of a word is 'Culture'. This word may or may not be followed by any other word. Then, there's a noun, regardless of its form. The query also constraints that the part of speech of 'Culture' should be the same as this noun.                                                    CQL: A:[lemma = 'Culture'] []? B:[pos = 'NN']  :: A.pos = B.pos                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          TIPS: You can use quantifiers in Regular Expression to match different number of times of a token. Here are some examples:                                                             *   Matches zero or more times.                                                                                                                                                    +   Matches one or more times.                                                                                                                                                     ?   Matches zero or one time.                                                                                                                                                      \{ n \}       Matches exactly n times.                                                                                                                                               \{ n ,\}      Matches at least n times.                                                                                                                                              \{ n , m \}   Matches from n to m times.                                                                                                              
% TIPS: You can use quantifiers in Regular Expression to match different number of times of a token. Here are some examples:                                                             *   Matches zero or more times.                                                                                                                                                    +   Matches one or more times.                                                                                                                                                     ?   Matches zero or one time.                                                                                                                                                      \{ n \}       Matches exactly n times.                                                                                                                                               \{ n ,\}      Matches at least n times.                                                                                                                                              \{ n , m \}   Matches from n to m times.                                                                                                                                                                                                                                                                                                                            TIPS: We use the The PTB part-of-speech system as the POS tag. The PTB system is as follows:                                                                                           CC: Coordinating conjunction                                                                                                                                                       CD: Cardinal number                                                                                                                                                                DT: Determiner                                                                                                                                                                     EX: Existential there                                                                                                                                                              FW: Foreign word                                                                                                                                                                   IN: Preposition or subordinating conjunction                                                                                                                                       JJ: Adjective                                                                                                                                                                      JJR:        Adjective, comparative                                                                                                                                                 JJS:        Adjective, superlative
%     LS: List item marker
%     MD: Modal
%     NN: Noun, singular or mass
%     NNS:        Noun, plural
%     NNP:        Proper noun, singular
%     NNPS:       Proper noun, plural
%     PDT:        Predeterminer
%     POS:        Possessive ending
%     PRP:        Personal pronoun
%     PRP\$:       Possessive pronoun
%     RB: Adverb
%     RBR:        Adverb, comparative
%     RBS:        Adverb, superlative
%     RP: Particle
%     SYM:        Symbol
%     TO: to
%     UH: Interjection
%     VB: Verb, base form
%     VBD:        Verb, past tense
%     VBG:        Verb, gerund or present participle
%     VBN:        Verb, past participle
%     VBP:        Verb, non-rd person singular present
%     VBZ:        Verb, rd person singular present
%     WDT:        Wh-determiner
%     WP: Wh-pronoun

%     Now please write a CQL to express the following requirement.
% Requirement: I am looking for sentences where a determiner is followed by one or more words and then by the word 'an'. However, the part of speech of 'an' should be different from that of the previously mentioned determiner.
% Give the CQL directly, do not attach other information.

% \subsection{3-Shot ICL (3SL) Prompts}

% You are a CQL (Corpus Query Language) expert. Now there is a corpus query requirement. Please express it as the corresponding CQL.                                                                                                                                                                                                                  Here are some examples to help you understand the requirements and CQL grammar:                                                                                                    Example 1:                                                                                                                                                                         Requirement: Search for instances where the word 'attentive' is followed by any form of the word 'staff'.                                                                          CQL: [word = 'attentive'] [lemma = 'staff']                                                                                                                                                                                                                                                                                                                           Example 2:                                                                                                                                                                         Requirement: Searching for an adverb followed by 3 to 9 arbitrary words, ending with the past participle verb 'booked'.                                                            CQL: [pos = 'RB'] []\{3,9\} [word = 'booked' & pos = 'VBN']                                                                                                                                                                                                                                                                                                             Example 3:                                                                                                                                                                         Requirement: Search for instances the lemma "awful" followed by the lemma "place" or any singular noun.                                                                            CQL: [lemma = 'awful'] [lemma = 'place' | pos = 'NN']                                                                                                                                                                                                                                                                                                                 Example 4:                                                                                                                                                                         Requirement: Find sentences where two consecutive adverbs that are not lemma "double" are followed by any word, and then the word "room".                                          CQL: [lemma != 'double' & pos = 'RB'] [lemma != 'double' & pos = 'RB'] [] [word = 'room']  within <s/>                                                                                                                                                                                                                                                                Example 5:                                                                                                                                                                         Requirement: Find comparative adjectives followed by any word ending with "hotel" within the range of an adjective that is not "special" and a word "thanks", allowing for 9 words in between.                                                                                                                                                                        CQL: [pos = 'JJR'] [word = '.*hotel']  within  [word != 'special' & pos = 'JJ'] []\{9\} [word = 'thanks']                                                                                                                                                                                                                                                               Example 6:                                                                                                                                                                         Requirement: Find sentences where the lemma 'cold' is immediately followed by the word 'food'.                                                                                     CQL: [lemma = 'cold'] [word = 'food']  within <s/>                                                                                                                                                                                                                                                                                                                    Example 7:                                                                                                                                                                         Requirement: The researcher is looking for instances where the lemma, or base form, of a word is 'Culture'. This word may or may not be followed by any other word. Then, there's a noun, regardless of its form. The query also constraints that the part of speech of 'Culture' should be the same as this noun.                                                    CQL: A:[lemma = 'Culture'] []? B:[pos = 'NN']  :: A.pos = B.pos                                                                                                                                                                                                                                                                                                       Example 8:                                                                                                                                                                         Requirement: I am looking for instances in the corpus where the lemma 'McKibbin' is followed by zero or one arbitrary words, and then the word 'Rehab'. Additionally, the part of speech of 'McKibbin' and 'Rehab' need to be the same.                                                                                                                               CQL: A:[lemma = 'McKibbin'] []? B:[word = 'Rehab']  :: A.pos = B.pos 

% Example 9:
% Requirement: This CQL is used to search for instances in sentences ('s' tag) where a word 'A' is followed by 3 to 8 arbitrary words and then followed by another word 'B', and 'A' and 'B' have the same part of speech (pos).
% CQL: A:[] []\{3,8\} B:[] within <s/> :: A.pos = B.pos


% TIPS: You can use quantifiers in Regular Expression to match different number of times of a token. Here are some examples:
%     *   Matches zero or more times.
%     +   Matches one or more times.
%     ?   Matches zero or one time.
%     \{ n \}       Matches exactly n times.
%     \{ n ,\}      Matches at least n times.
%     \{ n , m \}   Matches from n to m times.

%                                                                                                                                                                                    TIPS: We use the The PTB part-of-speech system as the POS tag. The PTB system is as follows:                                                                                           CC: Coordinating conjunction                                                                                                                                                       CD: Cardinal number                                                                                                                                                                DT: Determiner                                                                                                                                                                     EX: Existential there
%     FW: Foreign word
%     IN: Preposition or subordinating conjunction
%     JJ: Adjective
%     JJR:        Adjective, comparative
%     JJS:        Adjective, superlative
%     LS: List item marker
%     MD: Modal
%     NN: Noun, singular or mass
%     NNS:        Noun, plural
%     NNP:        Proper noun, singular
%     NNPS:       Proper noun, plural
%     PDT:        Predeterminer
%     POS:        Possessive ending
%     PRP:        Personal pronoun
%     PRP$:       Possessive pronoun
%     RB: Adverb
%     RBR:        Adverb, comparative
%     RBS:        Adverb, superlative
%     RP: Particle
%     SYM:        Symbol
%     TO: to
%     UH: Interjection
%     VB: Verb, base form
%     VBD:        Verb, past tense
%     VBG:        Verb, gerund or present participle
%     VBN:        Verb, past participle
%     VBP:        Verb, non-rd person singular present
%     VBZ:        Verb, rd person singular present
%     WDT:        Wh-determiner
%     WP: Wh-pronoun
%     WP$:        Possessive wh-pronoun
%     WRB:        Wh-adverb


% Now please write a CQL to express the following requirement.
% Requirement: I am looking for sentences where a determiner is followed by one or more words and then by the word 'an'. However, the part of speech of 'an' should be different from that of the previously mentioned determiner.

% Give the CQL directly, do not attach other information.
% \section{Example Tables}

% \begin{table}
%     \centering
%     \begin{tabular}{lp{9em}p{9em}} 
%     \hline
%     \textbf{Collocation Text} & \textbf{Collocation Type} \\
%     \hline
%     所以说\_AD	X	有\_VE	&D\_X\_V \\
%     各\_AD	有\_VE	& D\_V \\
%     有\_VE	利弊\_NN	& V\_O \\ 
%     \hline
%     \end{tabular}
%     \caption{A simple example of collocation extraction of the input sentence `
%     所以说各有利弊吧。(So there are pros and cons.)'. Input a natural language sentence, and the collocation extraction program gives a set of word combinations from the sentence. There is a dependency-based relationship between these word combinations. The X in the first collocation means there are other words between the two collocation words.}
%     \label{tab:collexample}
% \end{table}

% \begin{table}
%     \centering
%     \begin{tabular}{|p{3em}|p{15em}|} % 第一列最大宽度为四个字母
%     \hline
%     \textbf{NL}&  Find sentences where the word `evening' is followed by a noun, which occurred within a range of 4 to 7 words post adverb that is not word `real', before a noun that is not word `downside'. \\
%     \hline
%     \textbf{CQL} &[word = `evening'] [pos = `NN']  within  [word != `real' \& pos = `RB'] []\{4,7\} [word != `downside' \& pos = `NN'] within  <s/>   \\
%     \hline
    
%     \end{tabular}
%     \caption{Examples of generated \emph{within} CQL. A CQL with the condition keyword may have a long query length.}
%     \label{tab:within_example}
% \end{table}


% \begin{table}
%     \centering
%     \begin{tabular}{|p{3em}|p{15em}|} % 第一列最大宽度为四个字母
%     \hline
%     NL & Find corpus in which there are zero or one arbitrary words between two words of the same part of speech in a sentence. \\
%     \hline
%     CQL & A:[] []? B:[] within <s/> :: A.pos = B.pos \\
%     \hline
    
%     \end{tabular}
%     \caption{Examples of generated \emph{condition} CQL.}
%     \label{tab:condition_example}
% \end{table}
% \section{Dataset Details}
% \label{appdix:dataset}
% \begin{table}[h!]
% \centering
% \begin{tabular}{lccc}
% \hline
%  & \textbf{Train} & \textbf{Dev} & \textbf{Test} \\ \hline
% \textbf{Simple} & 2,836 & 405 & 811 \\
% \textbf{Within} & 1400 & 200 & 401 \\
% \textbf{Condition} & 699 & 99 & 201 \\
% \textbf{All} & 4,935 & 704 & 1,413 \\ \hline
% \end{tabular}
% \caption{Classification statistics for CQL datasets generated on our English corpus (EnWiki).}
% \label{tab:datasetdivide}
% \end{table}

% \begin{table}[h!]
% \centering
% \begin{tabular}{lccc}
% \hline
%  & \textbf{Train} & \textbf{Dev} & \textbf{Test} \\ \hline
% \textbf{Simple} & 2,795 & 390 & 798 \\
% \textbf{Within} & 932 & 134 & 266 \\
% \textbf{Condition} & 700 & 100 & 200 \\
% \textbf{All} & 4,427 & 624 & 1,264 \\ \hline
% \end{tabular}
% \caption{Classification statistics for CQL datasets generated on our Chinese corpus (TCFL). The split ratio of the train, dev, and test sets was divided according to 7:1:2, and checks were performed to remove low-quality data from the test set. We adopt the same division and check method on both corpora.}
% \label{tab:datasetdivide}
% \end{table}

% \begin{table}[h!]
% \centering
% \begin{tabular}{p{3em}lllll}
% \hline
% \textbf{Split} & $I_L$ & $Q_L$ & $Q_{TN}$ & $Q_{CN}$ & $Q_C$ \\
% \hline
% Simple    & 34.83 & 42.66 & 2.52 & 3.08 & 6.67 \\
% Within    & 39.74& 65.03 & 2.84 & 3.77 & 7.86 \\
% Condition & 70.71 & 53.47 & 3.44 & 2.27 & 8.95 \\
% \hline
% \end{tabular}
% \caption{More features of the Chinese part of the TCQL dataset. Where $I_L$ refers to the average character length of natural language requests, $Q_L$ refers to the average character length of CQL statements, $Q_{TN}$ is the average number of querying words of CQL statements, & $Q_{CN}$ is the average condition numbers, & $Q_C$ is the average number of tree depth in the AST.}
% \label{tab:zhstat}
% \end{table}

% \begin{table}[h!]
% \centering
% \begin{tabular}{p{3em}lllll}
% \hline
% \textbf{Split} & $I_L$ & $Q_L$ & $Q_{TN}$ & $Q_{CN}$ & $Q_C$ \\
% \hline
% Simple    & 18.62	&49.55	&2.50	&2.97	&6.58\\
% Within    & 21.24	&76.86	&2.89	&3.84	&8.07 \\
% Condition & 56.51	&63.70	&3.62	&2.74	&9.32\\
% \hline
% \end{tabular}
% \caption{More features of the English part of the TCQL dataset. Where $I_L$ refers to the average character length of natural language requests, $Q_L$ refers to the average character length of CQL statements, $Q_{TN}$ is the average number of querying words of CQL statements, & $Q_{CN}$ is the average condition numbers, & $Q_C$ is the average number of tree depth in the AST.}
% \label{tab:enstat}
% \end{table}

% \subsection{More results}
% \label{appendix:more}
% As shown in the table, the BART-Large model achieves the best results using Prefix-tuning in Chinese, while full model fine-tuning works better in English.
% \begin{table}[h!]
% \centering
% \begin{tabular}{p{8em}cc}
% \toprule
%  Fine-tuning Setting& \multicolumn{2}{c}{EM} \\
%  & TCQL-En & TCQL-Zh \\
% \midrule
% Prefix-tuning & 0.3510 & 0.4556 \\
% Full Model & 0.3758 & 0.4359 \\
% \bottomrule
% \end{tabular}
% \caption{Comparison of scores for the two fine-tuning methods. The scores in the table are Exact Match (EM) scores. After pre-testing, we have selected the method with the highest EM score for fine-tuning in Chinese and English respectively.}
% \label{tab:morePLM}
% \end{table}


\end{CJK*}
\end{document}

